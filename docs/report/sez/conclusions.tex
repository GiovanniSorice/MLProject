\section{Conclusions}
The project gave us the understanding of how the theory seen during the course is strictly correlated to the implementation of a real neural network library. Thanks to MONK's dataset we discovered how really the regularization techniques prevent overfitting, controlling model complexity. We understood the importance of choosing a good model and how to do it using grid search and k-cross validation. The hyper-parameter tuning was also the most difficult phase with the debugging and testing of the backpropagation algorithm. This experience thought us the foundations of learning algorithms on neural networks on which many machine learning frameworks are currently based. Overall this was a great experience where us, as a team, helped each other when the struggles came in.
\subsection{Agreement}
We agree to the disclosure and publication of my name, and of the results with the
preliminary and final ranking.


\begin{thebibliography}{9}
	\bibitem{monk} 
	S.B. Thrun, J. Bala, E. Boloederon and I. Bratko.
	\textit{The MONK's Problems}. 
	[\textit{A performance comparison of different learning algoritms}]. Chapter 9.
	Carnegie Mellon University CMU-CS-91-197, 1991.
	\\\texttt{https://pdfs.semanticscholar.org/94c0/418c4bd9d719e1203acfd42741ebbd343073.pdf}
	
	\bibitem{armadillo} 
	Conrad Sanderson and Ryan Curtin. 
	\textit{Armadillo: a template-based C++ library for linear algebra}. 
	\\\texttt{http://arma.sourceforge.net/armadillo\_joss\_2016.pdf}

	
	\bibitem{haykin} 
	Simon Haykin. 
	\textit{Neural Networks and Learning Machines}. 
	Prentice-Hall, (3rd Edition, 2008).


	\bibitem{mitchell} 
	T. M. Mitchell. 
	\textit{Machine learning}. 
	McGraw-Hill, 1997.
	
	\bibitem{goodfellow} 
	I. Goodfellow, Y. Bengio, A. Courville. 
	\textit{Deep Learning}. 
	MIT Press,  2016.
	
	\bibitem{michael} 
	Michael A. Nielsen. 
	\textit{Neural Networks and Deep Learning}. 
	Determination Press, 2015.
	\\\texttt{http://neuralnetworksanddeeplearning.com/chap2.html}
\end{thebibliography}



